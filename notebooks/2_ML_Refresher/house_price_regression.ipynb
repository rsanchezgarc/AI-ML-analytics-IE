{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 5407,
          "databundleVersionId": 868283,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 30822,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "notebook44cdf7aa95",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rsanchezgarc/AI-ML-analytics-IE/blob/main/notebooks/2_ML_Refresher/house_price_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first cell is included to automatically import the data from kaggle. But requires configuration, so you might prefer to skip it, and download the data manually."
      ],
      "metadata": {
        "id": "eQ6i_s-ROs1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"KAGGLE_USERNAME\"] = \"rsancg00\"\n",
        "os.environ[\"KAGGLE_KEY\"] = \"KGAT_39b4b30d8723b7a880ab8d31c3f836f6\"\n",
        "\n",
        "!kaggle competitions download -c house-prices-advanced-regression-techniques\n",
        "!unzip house-prices-advanced-regression-techniques.zip"
      ],
      "metadata": {
        "id": "t7RfbAJD_hBt",
        "outputId": "e62beb83-760f-4bfa-97c0-162c1c08c126",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading house-prices-advanced-regression-techniques.zip to /content\n",
            "\r  0% 0.00/199k [00:00<?, ?B/s]\n",
            "\r100% 199k/199k [00:00<00:00, 280MB/s]\n",
            "Archive:  house-prices-advanced-regression-techniques.zip\n",
            "  inflating: data_description.txt    \n",
            "  inflating: sample_submission.csv   \n",
            "  inflating: test.csv                \n",
            "  inflating: train.csv               \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-15T11:31:45.627043Z",
          "iopub.execute_input": "2025-01-15T11:31:45.627459Z",
          "iopub.status.idle": "2025-01-15T11:31:45.634788Z",
          "shell.execute_reply.started": "2025-01-15T11:31:45.627396Z",
          "shell.execute_reply": "2025-01-15T11:31:45.63316Z"
        },
        "id": "7_Gcj0LgxQia",
        "outputId": "24913ac6-f57d-477c-8674-b8f123122a83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3361798693.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/seaborn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcategorical\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F401,F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F401,F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F401,F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmiscplot\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F401,F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0maxisgrid\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F401,F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/seaborn/matrix.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhierarchy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0m_no_scipy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/cluster/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0m__all__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'vq'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'hierarchy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhierarchy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_testutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPytestTester\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/cluster/hierarchy.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   1908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1910\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mxp_capabilities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1911\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mto_mlab_linkage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1912\u001b[0m     \"\"\"\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/_lib/_array_api.py\u001b[0m in \u001b[0;36mdecorator\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    809\u001b[0m         \u001b[0mcapabilities_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcapabilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m         \u001b[0mnote\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_capabilities_note\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msphinx_capabilities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFunctionDoc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m         \u001b[0mdoc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Notes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnote\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# remove signature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/_lib/_docscrape.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, role, doc, config)\u001b[0m\n\u001b[1;32m    584\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No function or docstring given\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetdoc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/inspect.py\u001b[0m in \u001b[0;36mgetdoc\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m    878\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcleandoc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcleandoc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/inspect.py\u001b[0m in \u001b[0;36mcleandoc\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    901\u001b[0m             \u001b[0mlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmargin\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxsize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmargin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m         \u001b[0;31m# Remove any trailing or leading blank lines.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mlines\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = 'train.csv'\n",
        "test_path = 'test.csv'\n",
        "data_description_path = 'data_description.txt'\n",
        "sample_submission_path = 'sample_submission.csv'\n",
        "\n",
        "# Load the train dataset\n",
        "data = pd.read_csv(train_path)\n",
        "\n",
        "# Display the first few rows\n",
        "data.head()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-15T11:31:47.852992Z",
          "iopub.execute_input": "2025-01-15T11:31:47.853463Z",
          "iopub.status.idle": "2025-01-15T11:31:47.898676Z",
          "shell.execute_reply.started": "2025-01-15T11:31:47.85339Z",
          "shell.execute_reply": "2025-01-15T11:31:47.897555Z"
        },
        "id": "JyfAJlOXxQia"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "missing_values = data.isnull().sum().sort_values(ascending=False)\n",
        "print(\"Missing values per column:\")\n",
        "print(missing_values[missing_values > 0])\n",
        "\n",
        "# Visualize missing values\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.heatmap(data.isnull(), cbar=False, cmap='viridis')\n",
        "plt.title('Missing Data Heatmap')\n",
        "plt.show()\n",
        "\n",
        "print(\"N elements \", len(data))\n",
        "# Display summary statistics\n",
        "print(data.describe(include='all').T)\n",
        "#Remove columns with more than 400 missing NaNs\n",
        "good_colum_names = [data.columns[i] for i,count in enumerate(missing_values) if count<400]\n",
        "data = data[good_colum_names]"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-15T11:31:49.85116Z",
          "iopub.execute_input": "2025-01-15T11:31:49.851538Z",
          "iopub.status.idle": "2025-01-15T11:31:50.791808Z",
          "shell.execute_reply.started": "2025-01-15T11:31:49.85151Z",
          "shell.execute_reply": "2025-01-15T11:31:50.790724Z"
        },
        "id": "N27BrEUDxQib"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "num_imputer = SimpleImputer(strategy='median')\n",
        "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
        "\n",
        "# Separate numerical and categorical columns\n",
        "num_cols = data.select_dtypes(include=['float64', 'int64']).columns\n",
        "cat_cols = data.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Impute missing values\n",
        "data[num_cols] = num_imputer.fit_transform(data[num_cols])\n",
        "data[cat_cols] = cat_imputer.fit_transform(data[cat_cols])\n",
        "\n",
        "# Verify no missing values remain\n",
        "print(\"Remaining missing values per column:\")\n",
        "print(data.isnull().sum().sum())\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-15T11:31:54.901553Z",
          "iopub.execute_input": "2025-01-15T11:31:54.901934Z",
          "iopub.status.idle": "2025-01-15T11:31:54.952163Z",
          "shell.execute_reply.started": "2025-01-15T11:31:54.901904Z",
          "shell.execute_reply": "2025-01-15T11:31:54.951093Z"
        },
        "id": "0etHqql5xQib"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply label encoding to categorical columns\n",
        "label_encoders = {}\n",
        "for col in cat_cols:\n",
        "    le = LabelEncoder()\n",
        "    data[col] = le.fit_transform(data[col])\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# Display encoded data sample\n",
        "data.head()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-15T11:31:58.126701Z",
          "iopub.execute_input": "2025-01-15T11:31:58.127038Z",
          "iopub.status.idle": "2025-01-15T11:31:58.168994Z",
          "shell.execute_reply.started": "2025-01-15T11:31:58.127013Z",
          "shell.execute_reply": "2025-01-15T11:31:58.168065Z"
        },
        "id": "BJETVHOTxQib"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "target = 'SalePrice'\n",
        "X = data.drop(columns=[target])\n",
        "y = data[target]\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Verify the split\n",
        "print(\"Training data shape:\", X_train.shape)\n",
        "print(\"Validation data shape:\", X_val.shape)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-15T11:32:00.439599Z",
          "iopub.execute_input": "2025-01-15T11:32:00.439944Z",
          "iopub.status.idle": "2025-01-15T11:32:00.459008Z",
          "shell.execute_reply.started": "2025-01-15T11:32:00.439918Z",
          "shell.execute_reply": "2025-01-15T11:32:00.45793Z"
        },
        "id": "J2R7qeVsxQib"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is something important that I am not doing here. Feature scaling. I am not doing it because random forests can handle different scales for different features, but in things like a linear model or a deep learning model, feature scaling (aka, input normalization), is critical"
      ],
      "metadata": {
        "id": "hEsRcmTIxQic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "rf = RandomForestRegressor(random_state=42, n_estimators=100)\n",
        "\n",
        "# Train the model\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Make validation predictions\n",
        "y_pred = rf.predict(X_val)\n",
        "\n",
        "# Evaluate performance\n",
        "mse = mean_squared_error(y_val, y_pred)\n",
        "r2 = r2_score(y_val, y_pred)\n",
        "\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "print(f\"R^2 Score: {r2}\")\n",
        "\n",
        "# Plot true vs predicted values\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(y_val, y_pred, alpha=0.7)\n",
        "plt.xlabel(\"True Values\")\n",
        "plt.ylabel(\"Predicted Values\")\n",
        "plt.title(\"True vs Predicted Values\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-15T11:32:03.636455Z",
          "iopub.execute_input": "2025-01-15T11:32:03.636825Z",
          "iopub.status.idle": "2025-01-15T11:32:05.721288Z",
          "shell.execute_reply.started": "2025-01-15T11:32:03.636797Z",
          "shell.execute_reply": "2025-01-15T11:32:05.720072Z"
        },
        "id": "TJDaZIiXxQic"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Get feature importances\n",
        "feature_importances = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': rf.feature_importances_\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Display feature importances\n",
        "print(\"Feature Importances:\")\n",
        "print(feature_importances)\n",
        "\n",
        "# Plot feature importances\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.barh(feature_importances['Feature'], feature_importances['Importance'], color='skyblue')\n",
        "plt.xlabel('Importance')\n",
        "plt.ylabel('Feature')\n",
        "plt.title('Feature Importances')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-15T11:32:06.699338Z",
          "iopub.execute_input": "2025-01-15T11:32:06.69972Z",
          "iopub.status.idle": "2025-01-15T11:32:07.517492Z",
          "shell.execute_reply.started": "2025-01-15T11:32:06.699691Z",
          "shell.execute_reply": "2025-01-15T11:32:07.515905Z"
        },
        "id": "RRCX-ZVLxQic"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Id,SalePrice\n",
        "1461,169000.1\n",
        "1462,187724.1233\n",
        "1463,175221\n",
        "etc.\n",
        "\"\"\"\n",
        "\n",
        "def predict_test_set(model):\n",
        "    test_data = pd.read_csv(test_path)\n",
        "    ids = test_data[\"Id\"]\n",
        "    test_data[target] = 0 #Adding the target colum because the inputers where fitted with it\n",
        "    test_data = test_data[good_colum_names] #good_colum_names used in the training process\n",
        "\n",
        "    print(num_imputer.feature_names_in_)\n",
        "    test_data[num_cols] = num_imputer.transform(test_data[num_cols])\n",
        "    test_data[cat_cols] = cat_imputer.transform(test_data[cat_cols])\n",
        "    for col in cat_cols:\n",
        "        le = label_encoders[col]\n",
        "        test_data[col] = le.transform(test_data[col])\n",
        "    del test_data[target]\n",
        "\n",
        "    # Make predictions on the test data\n",
        "    test_predictions = model.predict(test_data)\n",
        "    # Create a submission DataFrame\n",
        "    submission_df = pd.DataFrame({'Id': ids.astype(np.int64), 'SalePrice': test_predictions})\n",
        "\n",
        "    # Save the submission DataFrame to a csv file\n",
        "    submission_df.to_csv('submission.csv', index=False)\n",
        "\n",
        "    # Display the first few rows of the submission file\n",
        "    print(submission_df.head().to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
        "\n",
        "predict_test_set(rf)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-15T11:32:33.862611Z",
          "iopub.execute_input": "2025-01-15T11:32:33.862962Z",
          "iopub.status.idle": "2025-01-15T11:32:33.967154Z",
          "shell.execute_reply.started": "2025-01-15T11:32:33.862935Z",
          "shell.execute_reply": "2025-01-15T11:32:33.96626Z"
        },
        "id": "-ZiLgCxsxQid"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since there is people that asked, I also added an example of anomaly/outliers detection using a non supervised method, the isolation forest. In many cases,it makes sense to remove outliers before training a supervised model. It is much better if the outlier removal method and the supervised method are based on different principles."
      ],
      "metadata": {
        "id": "2ICC5ACCxQid"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Outlier detection with isolation forest\n",
        "\n",
        "from sklearn.ensemble import IsolationForest\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming X_train and y_train are your feature and target sets\n",
        "\n",
        "# Combine X_train and y_train into a single DataFrame for easier processing\n",
        "data = pd.concat([X_train, y_train], axis=1)\n",
        "\n",
        "# Initialize the Isolation Forest model\n",
        "iso_forest = IsolationForest(contamination=0.05, random_state=42)  # Set contamination to 5% for example\n",
        "\n",
        "# Fit the model to the data (X_train + y_train)\n",
        "iso_forest.fit(data)\n",
        "\n",
        "# Get the predictions (-1 indicates outliers, 1 indicates inliers)\n",
        "predictions = iso_forest.predict(data)\n",
        "\n",
        "# Keep only the inliers (where prediction is 1)\n",
        "inliers = predictions == 1\n",
        "\n",
        "# Filter out the outliers from both X_train and y_train\n",
        "X_train_filtered = X_train[inliers]\n",
        "y_train_filtered = y_train[inliers]\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-15T11:32:40.68405Z",
          "iopub.execute_input": "2025-01-15T11:32:40.684998Z",
          "iopub.status.idle": "2025-01-15T11:32:41.058107Z",
          "shell.execute_reply.started": "2025-01-15T11:32:40.684942Z",
          "shell.execute_reply": "2025-01-15T11:32:41.056798Z"
        },
        "id": "cOI8VjJ8xQid"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#Outlier detection with kNN\n",
        "\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Combine X_train and y_train into a single DataFrame for easier processing\n",
        "data = pd.concat([X_train, y_train], axis=1)\n",
        "\n",
        "# Initialize the kNN model (you can adjust the number of neighbors)\n",
        "knn = NearestNeighbors(n_neighbors=5)  # Using 5 nearest neighbors, for example\n",
        "\n",
        "# Fit the model to the data\n",
        "knn.fit(data)\n",
        "\n",
        "# Get the distances and indices of the nearest neighbors\n",
        "distances, indices = knn.kneighbors(data)\n",
        "\n",
        "# Calculate the mean distance to the nearest neighbors for each data point\n",
        "mean_distances = np.mean(distances, axis=1)\n",
        "\n",
        "# Set a threshold for the maximum acceptable distance to consider a point as an outlier\n",
        "threshold = np.percentile(mean_distances, 95)  # For example, consider points in the top 5% of distances as outliers\n",
        "\n",
        "# Identify outliers: if mean distance is greater than threshold, it's an outlier\n",
        "outliers = mean_distances > threshold\n",
        "\n",
        "# Filter out the outliers from both X_train and y_train\n",
        "X_train_filtered = X_train[~outliers]\n",
        "y_train_filtered = y_train[~outliers]\n",
        "print(len(y_train_filtered), len(y_train))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-15T11:33:15.099844Z",
          "iopub.execute_input": "2025-01-15T11:33:15.100301Z",
          "iopub.status.idle": "2025-01-15T11:33:15.145723Z",
          "shell.execute_reply.started": "2025-01-15T11:33:15.100268Z",
          "shell.execute_reply": "2025-01-15T11:33:15.143507Z"
        },
        "id": "JPGpkHI8xQid"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can now try if using X_train_filtered is a good idea or not"
      ],
      "metadata": {
        "id": "E7nFYOg6xQid"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "rf = RandomForestRegressor(random_state=42, n_estimators=100)\n",
        "\n",
        "# Train the model\n",
        "rf.fit(X_train_filtered, y_train_filtered)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = rf.predict(X_val)\n",
        "\n",
        "# Evaluate performance\n",
        "mse = mean_squared_error(y_val, y_pred)\n",
        "r2 = r2_score(y_val, y_pred)\n",
        "\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "print(f\"R^2 Score: {r2}\")\n",
        "\n",
        "# Plot true vs predicted values\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(y_val, y_pred, alpha=0.7)\n",
        "plt.xlabel(\"True Values\")\n",
        "plt.ylabel(\"Predicted Values\")\n",
        "plt.title(\"True vs Predicted Values\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-15T11:33:25.805463Z",
          "iopub.execute_input": "2025-01-15T11:33:25.806114Z",
          "iopub.status.idle": "2025-01-15T11:33:27.807781Z",
          "shell.execute_reply.started": "2025-01-15T11:33:25.806065Z",
          "shell.execute_reply": "2025-01-15T11:33:27.80664Z"
        },
        "id": "tI6HH-ozxQid"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is an example of how to do automatic hyperparameter tuning"
      ],
      "metadata": {
        "id": "eARDSJtbxQid"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "collapsed": true,
        "id": "FYiQXOSGZeB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from sklearn.ensemble import RandomForestRegressor  # Use RandomForestClassifier if it's a classification problem\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.datasets import make_regression  # Use your own X_train and y_train here\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Define the objective function for Optuna. This will contain the full training\n",
        "# and scoring of the validation set process.\n",
        "def objective(trial):\n",
        "    # Hyperparameter search space\n",
        "    param_grid = {\n",
        "        'n_estimators': 200,  # Number of trees in the forest\n",
        "        'max_depth': trial.suggest_int('max_depth', 2, 50),  # Maximum depth of trees\n",
        "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),  # Minimum samples at a leaf node\n",
        "        'max_features': trial.suggest_categorical('max_features', [1.0, 'sqrt', 'log2']),  # Maximum number of features to consider,\n",
        "    }\n",
        "\n",
        "    # Initialize the RandomForestRegressor with the suggested hyperparameters\n",
        "    model = RandomForestRegressor(**param_grid, random_state=42)\n",
        "\n",
        "    # Cross-validation score (using negative mean squared error as an example)\n",
        "    score = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
        "    print(score)\n",
        "    # We return the negative mean of the cross-validation score as the objective to minimize\n",
        "    return -score.mean()\n",
        "\n",
        "# Create a study object to optimize the objective function\n",
        "study = optuna.create_study(direction='minimize')  # Minimize negative mean squared error\n",
        "study.optimize(objective, n_trials=100)  # You can adjust n_trials based on your computational budget\n",
        "\n",
        "# Output the best hyperparameters found\n",
        "print(f\"Best hyperparameters: {study.best_params}\")\n",
        "\n",
        "# You can now use the best parameters to train your final model:\n",
        "best_params = study.best_params\n",
        "best_rf_model = RandomForestRegressor(**best_params, random_state=42)\n",
        "best_rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = best_rf_model.predict(X_val)\n",
        "\n",
        "# Evaluate performance\n",
        "mse = mean_squared_error(y_val, y_pred)\n",
        "r2 = r2_score(y_val, y_pred)\n",
        "\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "print(f\"R^2 Score: {r2}\")\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-15T11:33:38.587169Z",
          "iopub.execute_input": "2025-01-15T11:33:38.587566Z",
          "iopub.status.idle": "2025-01-15T11:39:59.256641Z",
          "shell.execute_reply.started": "2025-01-15T11:33:38.587533Z",
          "shell.execute_reply": "2025-01-15T11:39:59.255525Z"
        },
        "id": "DyqpdIm_xQid"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}